# P1.2 NamedSharding Application Helpers

## Objective
- Implement helpers that turn computed `PartitionSpec` trees into concrete array placements using `NamedSharding` and `jax.device_put`.
- Provide a default batch-spec generator that shards the leading batch dimension along the DP axis while replicating other dims.
- Validate shapes, structures, and error reporting to unblock Engine/compile integration in P1.3.

## Current Context Snapshot
- `notes/updated_plan.md` defines P1.2 scope: `apply_named_sharding(tree, mesh, spec_tree)` and `shard_batch_specs(batch_example) -> spec_tree` with DP defaults.
- `src/titanax/parallel/sharding.py`:
  - P1.1 is complete: `tree_paths`, `spec_for`, `build_param_specs` implemented and tested.
  - P1.2 stubs remain: `apply_named_sharding(...)` and `shard_batch_specs(...)` both raise `NotImplementedError`.
- Compatibility layer exists: `src/titanax/compat.py` exports `Mesh`, `PartitionSpec`, and `NamedSharding` for version-safe imports; use it per code style guidance.
- Types: `src/titanax/types.py` provides aliases for `Mesh`, `PartitionSpec`, `PyTree`, etc.
- Tests: `tests/unit/test_sharding_utils.py` includes placeholders for P1.2 test classes marked with `@pytest.mark.skip(reason="P1.2 pending")`.

## Deliverables
- Working implementations:
  - `apply_named_sharding(tree, mesh, spec_tree) -> PyTree`
  - `shard_batch_specs(batch_example: PyTree, dp_axis: AxisName) -> SpecTree`
- Robust validation and clear Titanax exceptions (`ShardingError`) for structure mismatches and placement failures.
- Unit tests for placement correctness and batch-spec generation; unskip P1.2 sections in `tests/unit/test_sharding_utils.py` and add concrete cases.
- Docstrings clarifying semantics, edge cases, and version compatibility notes.

## Work Breakdown & Technical Details

### 1) Implement `apply_named_sharding`
- Purpose: Convert an arbitrary pytree of arrays into device-backed `jax.Array`s with explicit `NamedSharding(mesh, spec)` per leaf.
- Inputs/outputs:
  - Inputs: `tree: PyTree`, `mesh: Mesh`, `spec_tree: PyTree[PartitionSpec]` (same structure as `tree`).
  - Output: New pytree with same structure; leaves that are array-like become `jax.Array` with `NamedSharding` placement. Non-array leaves are passed through unchanged (see Edge Cases).
- Imports/compatibility:
  - Use `from titanax.compat import NamedSharding, PartitionSpec` and `from titanax.types import Mesh`.
  - Use `import jax` for `jax.device_put` (stable API across versions).
- Structure validation:
  - Use `jax.tree_util.tree_structure` to compare `tree` and `spec_tree` treedefs; if different, raise `sharding_error("<root>", "structure mismatch between value tree and spec tree", suggestion="Build specs via build_param_specs(...) or ensure spec_tree mirrors the value structure")`.
  - When mapping leaves, prefer `tree_flatten_with_path` to get path strings via existing `_stringify_path` for contextual error messages.
- Leaf handling:
  - Accept leaves that are:
    - `jax.Array` or `numpy.ndarray` or JAX array-likes (objects exposing `.shape`/`.dtype`).
    - Python scalars or small tuples: pass through unchanged (no sharding) to avoid surprising implicit conversions.
  - For array-like leaves, construct `ns = NamedSharding(mesh, spec)` and return `jax.device_put(leaf, ns)`.
  - If `NamedSharding` or `PartitionSpec` is `None` at runtime (very old JAX): raise a `ShardingError` with suggestion to upgrade.
- Error handling:
  - Wrap placement errors with `sharding_error(path, str(e), suggestion="Check that spec dimensions refer to mesh axes and match array rank")`.
- Performance notes:
  - This is a one-time data placement helper, not in the step hot path. Favor clarity and diagnostics over micro-optimizations.

### 2) Implement `shard_batch_specs`
- Purpose: Produce default DP-oriented `PartitionSpec` trees for input batches without requiring user-written rules.
- Semantics:
  - For each array-like leaf `x` with `ndim == 0`: return `PartitionSpec()` (replicated scalar).
  - For each array-like leaf `x` with `ndim >= 1`: return `PartitionSpec(dp_axis, None, ..., None)` where there are `(ndim - 1)` trailing `None`s.
  - For non-array leaves: return `PartitionSpec()` (replicated). Keep function conservative to avoid surprising placements.
- Inputs/outputs:
  - Input: `batch_example: PyTree` (typically dictionaries of arrays like `{"images": [B,H,W,C], "labels": [B]}`) and `dp_axis: AxisName`.
  - Output: PyTree of `PartitionSpec` with identical structure.
- Implementation details:
  - Use `jax.tree_util.tree_map` to derive specs; avoid materializing large arraysâ€”only read `ndim` if available.
  - Support shape-only descriptors like `jax.ShapeDtypeStruct` by checking for `.shape`/`.ndim` attributes.

### 3) Unit tests (expand `tests/unit/test_sharding_utils.py`)
- Unskip P1.2 test classes and add concrete tests:
  - `apply_named_sharding` placement:
    - Build a small mesh using `MeshSpec(axes=("data","model"))` or `(\"data\",)` and `build()`; create small `jnp` arrays in a tree and a matching `spec_tree` with a mix of `PartitionSpec()` and `PartitionSpec("model", None)`.
    - Apply helper and assert:
      - Shapes and dtypes preserved.
      - Each leaf is a `jax.Array` with a `NamedSharding`-backed `leaf.sharding`.
      - `leaf.sharding.spec == expected_spec` (use direct equality where supported; otherwise compare stringified specs as a fallback).
    - Mismatch handling: pass a spec tree with a different structure and assert `ShardingError` with a helpful message.
  - `shard_batch_specs` defaults:
    - Provide a representative batch example (e.g., `{"images": jnp.zeros((8, 4)), "labels": jnp.zeros((8,)) , "meta": 1}`) and `dp_axis = "data"`.
    - Assert returned tree mirrors structure and yields `PartitionSpec("data", None)` for `[B,4]`, `PartitionSpec("data")` for `[B]`, and `PartitionSpec()` for non-array leaves.
- Mark tests with `pytest.importorskip("jax")` and keep data sizes tiny to run on single-CPU setups.

### 4) Edge cases and safeguards
- Single-device meshes: Named sharding still applies; placement results replicate semantics but tests must not assume multi-device hardware.
- DP/TP axis names not present in mesh: placement will fail in JAX; catch and wrap with `ShardingError` mentioning the offending path and axis.
- Non-leaf arrays embedded in objects: only leaves are processed; document that complex containers should be flattened before sharding.
- Zero-length batch dimension: specs are still derived the same; compatibility checks against mesh divisibility happen elsewhere (`MeshSpec.validate_compatibility`).

### 5) Docs, typing, and style
- Follow Titanax style: imports grouped by stdlib/third-party/local; heavy type annotations on public functions; raise `ShardingError` with action-oriented suggestions.
- Import sharding types from `titanax.compat`; avoid direct `jax.sharding` in implementation.
- Add docstrings explaining default DP behavior and how to override via explicit rules for TP/PP.

### 6) Validation & Tooling
- Run: `uv run python -m pytest tests/unit/test_sharding_utils.py` (now including P1.2 tests).
- Lint/type: `uv run mypy src/titanax/` and `uv run ruff check src/ tests/` and format with `uv run black src/ tests/`.

## Acceptance Criteria
- `apply_named_sharding` and `shard_batch_specs` are implemented in `src/titanax/parallel/sharding.py` with type annotations and docstrings; no longer raise `NotImplementedError`.
- For correct inputs, `apply_named_sharding` returns a pytree of `jax.Array` leaves placed according to `NamedSharding(mesh, spec)`; shapes/dtypes are preserved.
- Passing mismatched `tree`/`spec_tree` structures raises `ShardingError` with a clear suggestion to derive specs via `build_param_specs`.
- `shard_batch_specs` produces a spec tree that mirrors input structure with DP-on-leading-dim semantics; scalars and non-array leaves are replicated.
- Unit tests in `tests/unit/test_sharding_utils.py` cover both functions and pass locally on single-CPU environments.
- Code adheres to Titanax code style, using `titanax.compat` for JAX sharding types and raising Titanax exceptions with helpful guidance.

## Out of Scope / Follow-ups
- Engine/compile integration (`in_shardings`/`out_shardings`) is handled in P1.3.
- Checkpoint metadata and resharding policies are handled in P1.4.
- Advanced batch heuristics (e.g., non-leading batch dims) are intentionally deferred; defaults remain conservative.


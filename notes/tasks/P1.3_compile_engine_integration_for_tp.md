# P1.3 Compile/Engine Integration for TP

## Objective
- Automatically translate tensor-parallel PartitionSpec trees into `pjit` shardings during compilation so plans no longer require manual wiring.
- Enable the Engine to delay compilation until runtime structures are known, ensuring TP mesh/plan data feeds into sharding decisions without extra user input.
- Provide a reusable parameter sharding hook so optimizers, checkpointing, and later phases can rely on pre-placed arrays aligned with TP rules.

## Current Context Snapshot
- `notes/updated_plan.md:28` documents P1.3 scope: extend `compile_step_with_plan`, add lazy Engine compilation, and introduce `Engine.shard_params`, with tests in compile/engine suites.
- `src/titanax/exec/compile.py:57` currently only honours caller-supplied `in_shardings`/`out_shardings`; otherwise it falls back to inferred DP-only specs plus `shard_map`, ignoring TP rules entirely.
- `src/titanax/exec/engine.py:276` compiles step functions eagerly during `register_step_fn`, has no concept of param/batch spec trees, and exposes no helper for sharding parameters ahead of compilation.
- `src/titanax/parallel/sharding.py:123`–`src/titanax/parallel/sharding.py:208` now implement `build_param_specs`, `apply_named_sharding`, and `shard_batch_specs`, giving us the primitives needed to turn TP rules into concrete placements.
- `tests/unit/test_compile.py:1` and `tests/unit/test_engine.py:1` are DP-focused; there is no coverage asserting TP-aware shardings or lazy compilation behaviour yet.

## Deliverables
- Updated `compile_step_with_plan` that can accept optional `param_spec_tree`/`batch_spec_tree` inputs and auto-construct `pjit` shardings when the active Plan defines TP rules.
- Engine changes providing `Engine.shard_params`, deferring compilation until shape data is available when TP is enabled, and keeping existing decorator overrides working.
- Unit tests expanding compile and engine suites to assert correct sharding inference, lazy compilation trigger points, and graceful fallbacks when users pass explicit shardings.
- Documentation-style docstrings/comments clarifying the new flow and how TP interacts with DP defaults.

## Work Breakdown & Technical Details

### 1) Extend `compile_step_with_plan`
- Add optional `param_spec_tree`/`batch_spec_tree` parameters (plus possibly `out_spec_tree`) so callers can feed P1.2 outputs directly; default them to `None` for backward compatibility.
- When TP is present (`plan.tensor_parallel` not `None`) and explicit shardings were not supplied, use provided spec trees to build `in_shardings`/`out_shardings` tuples for `pjit`. Ensure state spec is derived from `param_spec_tree`, batch spec from `batch_spec_tree`, and metrics/default outputs remain replicated unless told otherwise.
- Validate that `param_spec_tree` mirrors the TrainState structure (state params/optimizer/rngs) before deriving shardings; surface mismatches via `CompilationError` with actionable suggestions.
- Maintain existing single-device and shard_map fallbacks; only switch execution path when spec trees and TP plan are available.

### 2) Derive spec trees from Plans
- Inside Engine (or helper), build parameter spec trees by calling `build_param_specs` with `plan.tensor_parallel.rules`; incorporate DP defaults when TP rules omit leaves (e.g., fall back to replicated specs).
- Generate batch spec trees using `shard_batch_specs` when DP is configured, then overlay TP dimensions when batch elements must respect model axis (e.g., activations produced by TP layers) — document assumptions and assert shape compatibility early.
- Store these trees on the Engine so they can be reused for parameter sharding, compilation, and checkpoint interactions without recomputation.

### 3) Introduce `Engine.shard_params` and lazy compilation
- Implement `Engine.shard_params(params)` to call `apply_named_sharding` with the cached `param_spec_tree`; ensure it is idempotent and raises clear errors if called before TP spec derivation.
- Update `register_step_fn` to stash the raw step function and compilation metadata but defer actual compilation when TP is active and `in_shardings`/`out_shardings` were not provided; compilation should occur when we first see both a `TrainState` and `BatchData` (e.g., at `Engine.step` or the beginning of `fit`).
- On deferred compile, construct `in_shardings` and `out_shardings` using the stored spec trees and pass them into `compile_step_with_plan`; respect decorator-overrides by bypassing lazy path when users already set shardings.
- Ensure metrics logging and checkpoint hooks still see compiled functions; guard concurrency by making lazy compilation thread-safe or clearly single-threaded within Engine usage.

### 4) Update compile & engine tests
- Add TP-oriented cases in `tests/unit/test_compile.py`: create a toy Plan with `tensor_parallel` rules, generate spec trees, call `compile_step_with_plan`, and assert that `pjit` runs with expected sharding (`jax.jit` fallback must not trigger).
- Extend `tests/unit/test_engine.py` with fixtures that build a TP plan/mesh, register a step function without explicit shardings, call `step/create_state`, and assert compilation happens lazily with correct shardings. Cover explicit-sharding override to ensure backward compatibility.
- Use small CPU meshes (e.g., 2-device host mesh) and `jax.ShapeDtypeStruct` to avoid heavy allocations; gate tests behind `pytest.importorskip("jax.experimental.pjit")` or similar if needed for compatibility.

### 5) Validation, typing, and docs
- Add docstrings to the new Engine helpers explaining lifecycle (when spec trees exist, how they interact with checkpointing).
- Ensure new arguments/types are reflected in `src/titanax/types.py` if necessary and update any re-export modules.
- Run the standard pre-commit suite (`mypy`, `ruff`, `black`, `pytest`) after implementation; include targeted pytest invocations in task notes for quick iteration.

## Acceptance Criteria
- `compile_step_with_plan` accepts spec trees, builds TP-aware `pjit` shardings when no explicit shardings were provided, and still supports DP-only or shard_map fallbacks without regressions.
- Engine exposes `shard_params` and defers compilation until runtime data is available when TP is enabled; first `step()` call compiles using derived shardings without user intervention.
- Updated tests in compile/engine modules cover TP sharding inference, lazy compilation, and decorator override paths, passing on CPU-only environments.
- Documentation and inline comments clarify the TP integration flow, and the code passes Titanax formatting, linting, and typing checks.
